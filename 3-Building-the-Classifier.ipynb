{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Î ÏÎ±ÏƒÎ¹Î½Î¿ Î±Ï…ÎºÎ¿Ï…Î¹ Î¼ÎµÏ‚ Ï„Î¿ Ï€Î±ÏƒÏ‡Î±Î»Î¹Î½Î¿ Ï€Î¿Ï„Î®ÏÎ¹ Ï€Î¿Ï… Î­Ï€Î¹Î±ÏƒÎµ Î¿ Î¼Î¹Ï„ÏƒÎ·Ï‚ #Î±Î¹ÏƒÏ‡Î¿Ï‚ ğŸ¤£ğŸ¤£ğŸ¤£   @HARRIS_APOEL https://t.co/y9X7CmBEa5',\n",
       " '@HARRIS_APOEL @pirpoitis @vassrules ÎšÎ±Î¼Î½Î¿Ï…Î½ Î±Î½Î±ÎºÎ±Î¹Î½Î¹ÏƒÎ· ÏƒÏ„Î± Î ÎµÏÎ²Î¿Î»Î¹Î± Ï†ÎµÏ„Î¿Ï‚.',\n",
       " '@MUFCChristian Î•Î»Î± ÏƒÏ…Î³Î³ÎµÎ½Î· Ï„Î¶Î¹Î±Î¹ ÎµÏ‡Î¿Ï…Î¼Îµ Î½ÎµÎ¿Ï„ÎµÏÎ± Ï€ Ï„Î¿ ÎÎ¹ÎºÎ¿Î»Î·.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "cg_sents = []\n",
    "smg_sents = []\n",
    "\n",
    "def remove_duplicate_punctuation(s): # sent_tokenize() gets confused when there's duplicate punctuation \n",
    "    return(re.sub(r'(\\.|\\?|!)\\1+', r'\\1', s))\n",
    "    \n",
    "with open('./Data/cg_twitter.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p] # sent_tokenize() doesn't consider a new line a new sentence so this is required.\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/cg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/cg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        cg_sents += sent_tokenize(line)\n",
    "\n",
    "with open('./Data/smg_twitter.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/smg_fb.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "    \n",
    "with open('./Data/smg_other.txt', 'r', encoding='utf-8') as in_file:\n",
    "    text = remove_duplicate_punctuation(in_file.read())\n",
    "    lines = [p for p in text.split('\\n') if p]\n",
    "    for line in lines:\n",
    "        smg_sents += sent_tokenize(line)\n",
    "\n",
    "cg_sents[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cleaning the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ï€ÏÎ±ÏƒÎ¹Î½Î¿ Î±Ï…ÎºÎ¿Ï…Î¹ Î¼ÎµÏ‚ Ï„Î¿ Ï€Î±ÏƒÏ‡Î±Î»Î¹Î½Î¿ Ï€Î¿Ï„Î·ÏÎ¹ Ï€Î¿Ï… ÎµÏ€Î¹Î±ÏƒÎµ Î¿ Î¼Î¹Ï„ÏƒÎ·Ï‚',\n",
       " 'ÎºÎ±Î¼Î½Î¿Ï…Î½ Î±Î½Î±ÎºÎ±Î¹Î½Î¹ÏƒÎ· ÏƒÏ„Î± Ï€ÎµÏÎ²Î¿Î»Î¹Î± Ï†ÎµÏ„Î¿Ï‚',\n",
       " 'ÎµÎ»Î± ÏƒÏ…Î³Î³ÎµÎ½Î· Ï„Î¶Î¹Î±Î¹ ÎµÏ‡Î¿Ï…Î¼Îµ Î½ÎµÎ¿Ï„ÎµÏÎ± Ï€ Ï„Î¿ Î½Î¹ÎºÎ¿Î»Î·']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "from string import punctuation\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "\n",
    "def strip_accents(s):\n",
    "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "                   if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "def get_clean_sent_el(sentence):\n",
    "    sentence = re.sub(r'^RT', '', sentence)\n",
    "    sentence = re.sub(r'\\&\\w*;', '', sentence)\n",
    "    sentence = re.sub(r'\\@\\w*', '', sentence)\n",
    "    sentence = re.sub(r'\\$\\w*', '', sentence)\n",
    "    sentence = re.sub(r'https?:\\/\\/.*\\/\\w*', '', sentence)\n",
    "    sentence = ''.join(c for c in sentence if c <= '\\uFFFF')\n",
    "    sentence = strip_accents(sentence)\n",
    "    sentence = re.sub(r'#\\w*', '', sentence)\n",
    "    tokens = WhitespaceTokenizer().tokenize(sentence)\n",
    "    new_tokens = []\n",
    "    for token in tokens:\n",
    "        if token == 'Î¿,Ï„Î¹' or token == 'ÏŒ,Ï„Î¹' or token == 'o,ti' or token == 'Ã³,ti':\n",
    "            new_tokens.append(token)\n",
    "        else:\n",
    "            new_tokens.append(token.translate(str.maketrans({key: None for key in punctuation})))\n",
    "    sentence =' '.join(new_tokens)\n",
    "    sentence = sentence.strip(' ') # performs lstrip() and rstrip()\n",
    "    return sentence.lower()\n",
    "\n",
    "cg_sents_clean = []\n",
    "smg_sents_clean = []\n",
    "\n",
    "for sent in cg_sents:\n",
    "    cg_sents_clean.append(get_clean_sent_el(sent))\n",
    "for sent in smg_sents:\n",
    "    smg_sents_clean.append(get_clean_sent_el(sent))\n",
    "\n",
    "# Remove empty strings left due to sentences ending up being only URLs then getting deleted on cleaning:\n",
    "cg_sents_clean = list(filter(None, cg_sents_clean))\n",
    "smg_sents_clean = list(filter(None, smg_sents_clean))\n",
    "cg_sents_clean[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Ï€ÏÎ±ÏƒÎ¹Î½Î¿',\n",
       "  'Î±Ï…ÎºÎ¿Ï…Î¹',\n",
       "  'Î¼ÎµÏ‚',\n",
       "  'Ï„Î¿',\n",
       "  'Ï€Î±ÏƒÏ‡Î±Î»Î¹Î½Î¿',\n",
       "  'Ï€Î¿Ï„Î·ÏÎ¹',\n",
       "  'Ï€Î¿Ï…',\n",
       "  'ÎµÏ€Î¹Î±ÏƒÎµ',\n",
       "  'Î¿',\n",
       "  'Î¼Î¹Ï„ÏƒÎ·Ï‚'],\n",
       " ['ÎºÎ±Î¼Î½Î¿Ï…Î½', 'Î±Î½Î±ÎºÎ±Î¹Î½Î¹ÏƒÎ·', 'ÏƒÏ„Î±', 'Ï€ÎµÏÎ²Î¿Î»Î¹Î±', 'Ï†ÎµÏ„Î¿Ï‚'],\n",
       " ['ÎµÎ»Î±', 'ÏƒÏ…Î³Î³ÎµÎ½Î·', 'Ï„Î¶Î¹Î±Î¹', 'ÎµÏ‡Î¿Ï…Î¼Îµ', 'Î½ÎµÎ¿Ï„ÎµÏÎ±', 'Ï€', 'Ï„Î¿', 'Î½Î¹ÎºÎ¿Î»Î·']]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cg_sents_tokens = []\n",
    "smg_sents_tokens = []\n",
    "\n",
    "for sent in cg_sents_clean:\n",
    "    cg_sents_tokens.append(WhitespaceTokenizer().tokenize(sent))\n",
    "for sent in smg_sents_clean:\n",
    "    smg_sents_tokens.append(WhitespaceTokenizer().tokenize(sent))\n",
    "    \n",
    "cg_sents_tokens[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Building the feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "def get_word_ngrams(tokens, n):\n",
    "    ngrams_list = []\n",
    "    ngrams_list.append(list(ngrams(tokens, n)))\n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = '%s'\n",
    "    for i in range(1, n):\n",
    "        format_string += (' %s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat\n",
    "\n",
    "def get_char_ngrams(word, n):\n",
    "    ngrams_list = []\n",
    "    word = re.sub(r'Ï‚', 'Ïƒ', word)\n",
    "    ngrams_list.append(list(ngrams(word, n, pad_left=True, pad_right=True, left_pad_symbol='_', right_pad_symbol='_')))\n",
    "    \n",
    "    # Removing redundant ngrams:\n",
    "    if (n > 2):\n",
    "        redundant_combinations = n - 2\n",
    "        ngrams_list = [ngram_list[redundant_combinations : -redundant_combinations] for ngram_list in ngrams_list]\n",
    "    \n",
    "    ngrams_flat_tuples = [ngram for ngram_list in ngrams_list for ngram in ngram_list]\n",
    "    format_string = ''\n",
    "    for i in range(0, n):\n",
    "        format_string += ('%s')\n",
    "    ngrams_list_flat = [format_string % ngram_tuple for ngram_tuple in ngrams_flat_tuples]\n",
    "    return ngrams_list_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'char(Î±)': 3,\n",
       " 'char(Îµ)': 1,\n",
       " 'char(Î·)': 3,\n",
       " 'char(Î¹)': 2,\n",
       " 'char(Î½)': 1,\n",
       " 'char(Ï€)': 1,\n",
       " 'char(Ï)': 1,\n",
       " 'char(Ïƒ)': 1,\n",
       " 'char(Ï„)': 2,\n",
       " 'char(Ï…)': 1,\n",
       " 'char_bigram(_Î±)': 1,\n",
       " 'char_bigram(_Îµ)': 1,\n",
       " 'char_bigram(_Î·)': 1,\n",
       " 'char_bigram(_Ïƒ)': 1,\n",
       " 'char_bigram(Î±Î¹)': 1,\n",
       " 'char_bigram(Î±Ï)': 1,\n",
       " 'char_bigram(Î±Ï…)': 1,\n",
       " 'char_bigram(ÎµÎ¹)': 1,\n",
       " 'char_bigram(Î·_)': 3,\n",
       " 'char_bigram(Î¹_)': 1,\n",
       " 'char_bigram(Î¹Î½)': 1,\n",
       " 'char_bigram(Î½Î±)': 1,\n",
       " 'char_bigram(Ï€Î±)': 1,\n",
       " 'char_bigram(ÏÏ„)': 1,\n",
       " 'char_bigram(ÏƒÏ€)': 1,\n",
       " 'char_bigram(Ï„Î·)': 2,\n",
       " 'char_bigram(Ï…Ï„)': 1,\n",
       " 'char_quadrigram(_Î±Ï…Ï„)': 1,\n",
       " 'char_quadrigram(_ÎµÎ¹Î½)': 1,\n",
       " 'char_quadrigram(_ÏƒÏ€Î±)': 1,\n",
       " 'char_quadrigram(Î±ÏÏ„Î·)': 1,\n",
       " 'char_quadrigram(Î±Ï…Ï„Î·)': 1,\n",
       " 'char_quadrigram(ÎµÎ¹Î½Î±)': 1,\n",
       " 'char_quadrigram(Î¹Î½Î±Î¹)': 1,\n",
       " 'char_quadrigram(Î½Î±Î¹_)': 1,\n",
       " 'char_quadrigram(Ï€Î±ÏÏ„)': 1,\n",
       " 'char_quadrigram(ÏÏ„Î·_)': 1,\n",
       " 'char_quadrigram(ÏƒÏ€Î±Ï)': 1,\n",
       " 'char_quadrigram(Ï…Ï„Î·_)': 1,\n",
       " 'char_trigram(_Î±Ï…)': 1,\n",
       " 'char_trigram(_ÎµÎ¹)': 1,\n",
       " 'char_trigram(_Î·_)': 1,\n",
       " 'char_trigram(_ÏƒÏ€)': 1,\n",
       " 'char_trigram(Î±Î¹_)': 1,\n",
       " 'char_trigram(Î±ÏÏ„)': 1,\n",
       " 'char_trigram(Î±Ï…Ï„)': 1,\n",
       " 'char_trigram(ÎµÎ¹Î½)': 1,\n",
       " 'char_trigram(Î¹Î½Î±)': 1,\n",
       " 'char_trigram(Î½Î±Î¹)': 1,\n",
       " 'char_trigram(Ï€Î±Ï)': 1,\n",
       " 'char_trigram(ÏÏ„Î·)': 1,\n",
       " 'char_trigram(ÏƒÏ€Î±)': 1,\n",
       " 'char_trigram(Ï„Î·_)': 2,\n",
       " 'char_trigram(Ï…Ï„Î·)': 1,\n",
       " 'word(Î±Ï…Ï„Î·)': 1,\n",
       " 'word(ÎµÎ¹Î½Î±Î¹)': 1,\n",
       " 'word(Î·)': 1,\n",
       " 'word(ÏƒÏ€Î±ÏÏ„Î·)': 1,\n",
       " 'word_bigram(Î±Ï…Ï„Î· ÎµÎ¹Î½Î±Î¹)': 1,\n",
       " 'word_bigram(ÎµÎ¹Î½Î±Î¹ Î·)': 1,\n",
       " 'word_bigram(Î· ÏƒÏ€Î±ÏÏ„Î·)': 1,\n",
       " 'word_quadrigram(Î±Ï…Ï„Î· ÎµÎ¹Î½Î±Î¹ Î· ÏƒÏ€Î±ÏÏ„Î·)': 1,\n",
       " 'word_trigram(Î±Ï…Ï„Î· ÎµÎ¹Î½Î±Î¹ Î·)': 1,\n",
       " 'word_trigram(ÎµÎ¹Î½Î±Î¹ Î· ÏƒÏ€Î±ÏÏ„Î·)': 1}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature extractor\n",
    "def get_ngram_features(sentence_tokens):\n",
    "    features = {}\n",
    "    \n",
    "    # Word unigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 1)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word({ngram})'] = features.get(f'word({ngram})', 0) + 1 # The second paramter to .get() is a default value if the key doesn't exist.\n",
    "    \n",
    "    # Word bigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 2)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_bigram({ngram})'] = features.get(f'word_bigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Word trigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 3)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_trigram({ngram})'] = features.get(f'word_trigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Word quadrigrams\n",
    "    ngrams = get_word_ngrams(sentence_tokens, 4)\n",
    "    for ngram in ngrams:\n",
    "        features[f'word_quadrigram({ngram})'] = features.get(f'word_quadrigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char unigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_word_ngrams(word, 1)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char({ngram})'] = features.get(f'char({ngram})', 0) + 1\n",
    "    \n",
    "    # Char bigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 2)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_bigram({ngram})'] = features.get(f'char_bigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char trigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 3)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_trigram({ngram})'] = features.get(f'char_trigram({ngram})', 0) + 1\n",
    "    \n",
    "    # Char quadrigrams\n",
    "    for word in sentence_tokens:\n",
    "        ngrams = get_char_ngrams(word, 4)\n",
    "        for ngram in ngrams:\n",
    "            features[f'char_quadrigram({ngram})'] = features.get(f'char_quadrigram({ngram})', 0) + 1\n",
    "    \n",
    "    return features\n",
    "\n",
    "get_ngram_features(['Î±Ï…Ï„Î·', 'ÎµÎ¹Î½Î±Î¹', 'Î·', 'ÏƒÏ€Î±ÏÏ„Î·'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Labeling the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Ï€ÏÎ±ÏƒÎ¹Î½Î¿',\n",
       "  'Î±Ï…ÎºÎ¿Ï…Î¹',\n",
       "  'Î¼ÎµÏ‚',\n",
       "  'Ï„Î¿',\n",
       "  'Ï€Î±ÏƒÏ‡Î±Î»Î¹Î½Î¿',\n",
       "  'Ï€Î¿Ï„Î·ÏÎ¹',\n",
       "  'Ï€Î¿Ï…',\n",
       "  'ÎµÏ€Î¹Î±ÏƒÎµ',\n",
       "  'Î¿',\n",
       "  'Î¼Î¹Ï„ÏƒÎ·Ï‚'],\n",
       " 'cg')"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cg_sents_features_labeled = [(get_ngram_features(word), 'cg') for word in cg_sents_tokens]\n",
    "# smg_sents_features_labeled = [(get_ngram_features(word), 'smg') for word in smg_sents_tokens]\n",
    "\n",
    "# all_sents_features_labeled = cg_sents_features_labeled + smg_sents_features_labeled\n",
    "# all_sents_features_labeled[0]\n",
    "\n",
    "all_sents_labeled = ([(sentence, 'cg') for sentence in cg_sents_tokens] + [(sentence, 'smg') for sentence in smg_sents_tokens])\n",
    "all_sents_labeled[0]                                      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Splitting corpus into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET\t SENTENCES\n",
      "All\t 154\n",
      "Training 123\n",
      "Testing\t 31\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk.classify import apply_features\n",
    "\n",
    "random.shuffle(all_sents_labeled)\n",
    "\n",
    "no_all_sentences = len(all_sents_labeled)\n",
    "no_train_sentences = round(no_all_sentences * .8)\n",
    "\n",
    "print('DATASET\\t', 'SENTENCES')\n",
    "print('All\\t', no_all_sentences)\n",
    "print('Training', no_train_sentences)\n",
    "print('Testing\\t', no_all_sentences - no_train_sentences)\n",
    "\n",
    "train_set = apply_features(get_ngram_features, all_sents_labeled[:no_train_sentences])\n",
    "test_set = apply_features(get_ngram_features, all_sents_labeled[no_train_sentences:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Building a Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hzsab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from nltk import NaiveBayesClassifier\n",
    "\n",
    "classifier = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cg', 'smg']"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\hzsab\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\ipykernel_launcher.py:5: DeprecationWarning: generator 'ngrams' raised StopIteration\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6451612903225806"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.classify import accuracy\n",
    "\n",
    "accuracy(classifier, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "         char_bigram(Ï‰Ï„) = 1                 smg : cg     =     13.5 : 1.0\n",
      "   char_quadrigram(Î¹Î½Î±Î¹) = 1                 smg : cg     =     11.9 : 1.0\n",
      "             word(ÎµÎ¹Î½Î±Î¹) = 1                 smg : cg     =     11.9 : 1.0\n",
      "   char_quadrigram(_ÎµÎ¹Î½) = 1                 smg : cg     =     11.9 : 1.0\n",
      "         char_bigram(Î»_) = 1                 smg : cg     =      8.7 : 1.0\n",
      "   char_quadrigram(ÎµÎ¹Î½Î±) = 1                 smg : cg     =      8.1 : 1.0\n",
      "         char_bigram(Î¹Ïƒ) = 2                 smg : cg     =      8.0 : 1.0\n",
      "         char_bigram(Ï‡Îµ) = 1                 smg : cg     =      7.1 : 1.0\n",
      "       char_trigram(Ï„Î±Î¹) = 1                 smg : cg     =      7.1 : 1.0\n",
      "   char_quadrigram(Î±Î»Î±_) = 1                 smg : cg     =      7.1 : 1.0\n"
     ]
    }
   ],
   "source": [
    "classifier.show_most_informative_features(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
